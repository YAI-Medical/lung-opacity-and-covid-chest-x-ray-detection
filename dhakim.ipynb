{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preparations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Mounting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Check, with local module, whether runtime is colaboratory\n",
    "\n",
    "try:  # local runtime\n",
    "    import library_check\n",
    "except ImportError:  # colab runtime\n",
    "    library_check = None\n",
    "    from google.colab import drive  # NOQA\n",
    "    drive.mount('/content/drive')  # NOQA\n",
    "    colaboratory = True\n",
    "else:\n",
    "    colaboratory = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Runtime Check"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS version: \t\tmacOS-11.5.2-arm64-arm-64bit\n",
      "Python version:\t\t3.8.10 | packaged by conda-forge | (default, May 11 2021, 06:27:18) \n",
      "[Clang 11.1.0 ]\n"
     ]
    }
   ],
   "source": [
    "# System Information\n",
    "\n",
    "import sys\n",
    "import platform\n",
    "print(f\"OS version: \\t\\t{platform.platform()}\\n\"\n",
    "      f\"Python version:\\t\\t{sys.version}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Library Installation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are installed.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "\n",
    "if colaboratory:  # colab runtime\n",
    "    !pip install pydicom\n",
    "    # !pip install mxnet-cu101==1.7.0 d2l==0.16.6\n",
    "    !git clone https://github.com/kdha0727/lung-opacity-and-covid-chest-x-ray-detection/\n",
    "    %cd lung-opacity-and-covid-chest-x-ray-detection\n",
    "    import library_check\n",
    "    library_check.check()\n",
    "    import data_prep_utils\n",
    "    root = \"/content/drive/Shareddrives/2021 하계 SAT/\"\n",
    "    data_prep_utils.set_root(root)\n",
    "else:  # local runtime\n",
    "    library_check.check()\n",
    "    import data_prep_utils\n",
    "data_prep_utils.init()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# After all installation, import all libraries used.\n",
    "\n",
    "import inspect\n",
    "import random\n",
    "import pydicom as dcm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import seaborn as sns\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch\n",
    "import torchvision\n",
    "import torchsummary\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "\n",
    "from timm.models.efficientnet import tf_efficientnet_b4\n",
    "from effdet import get_efficientdet_config, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet, BiFpn, _init_weight\n",
    "\n",
    "from skimage import io, transform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# And, import custom-defined Lazy Data Wrappers and Utilities\n",
    "\n",
    "from data_prep_utils import covid_19_radiography_dataset\n",
    "from data_prep_utils import rsna_pneumonia_detection_challenge\n",
    "\n",
    "from data_prep_utils.dataset import pil_loader, dicom_loader\n",
    "import train_utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis and Processing\n",
    "\n",
    "* Note: All preprocessing processes are modularized as \"Data Wrapper\" package"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Class Information**\n",
    "* Normal: 0\n",
    "* Lung Opacity: 1\n",
    "* COVID-19: 2\n",
    "* Viral Pneumonia: 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: [\n",
      "\t('Normal', 0)\n",
      "\t('Lung_Opacity', 1)\n",
      "\t('COVID', 2)\n",
      "\t('Viral Pneumonia', 3) \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\t'.join(map(str, ['Labels: [', *covid_19_radiography_dataset.class_to_idx.items()])), '\\n]')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modeling via PyTorch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ImageWithPandas(VisionDataset):\n",
      "    \"\"\"A generic data loader where the image path and label is given as pandas DataFrame.\n",
      "\n",
      "    Args:\n",
      "        dataframe (pandas.DataFrame): A data table that contains image path, target class,\n",
      "            and extra outputs.\n",
      "        label_id (string): Data frame`s image path label string.\n",
      "        label_target (string): Data frame`s target class label string.\n",
      "        label_extras (tuple[string] or string, optional): Data frame`s label that will\n",
      "            be used for extra outputs.\n",
      "        root (string, optional): Root directory path. Use unless data frame`s column\n",
      "            contains file folders.\n",
      "        extension (string, optional): An extension that will be concatenated after\n",
      "            image file name. Use unless data frame`s column contains extension.\n",
      "        class_to_idx (dict[str, int], optional): A mapping table that converts class\n",
      "            label string into integer value. If not given, sorted index value will\n",
      "            be used as class integer value.\n",
      "        transform (callable, optional): A function/transform that takes in an image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "        extras_transform (callable, optional): A function/transform that takes in the\n",
      "            extra outputs and transforms it.\n",
      "        loader (callable, optional): A function to load an image given its path.\n",
      "\n",
      "     Attributes:\n",
      "        classes (list): List of the class names sorted alphabetically.\n",
      "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "        samples (list): List of (sample path, class_index) tuples\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            dataframe: pd.DataFrame,\n",
      "            label_id: str,\n",
      "            label_target: str,\n",
      "            root: typing.Optional[typing.Union[str, os.PathLike]] = None,\n",
      "            extension: typing.Optional[str] = None,\n",
      "            class_to_idx: typing.Optional[typing.Dict[typing.Any, int]] = None,\n",
      "            transform: typing.Optional[typing.Callable] = None,\n",
      "            target_transform: typing.Optional[typing.Callable] = None,\n",
      "            extras_transform: typing.Optional[typing.Callable] = None,\n",
      "            loader: typing.Callable[[str], typing.Any] = default_loader,\n",
      "    ) -> None:\n",
      "\n",
      "        super(ImageWithPandas, self).__init__(root, None, transform, target_transform)\n",
      "\n",
      "        self.extras_transform = extras_transform\n",
      "        self.loader = loader\n",
      "        self.label_id = label_id\n",
      "        self.label_target = label_target\n",
      "\n",
      "        labels = [label_id, label_target]\n",
      "\n",
      "        samples = dataframe[labels].copy(deep=True)\n",
      "\n",
      "        assert extension.startswith('.') or extension is None\n",
      "        if root is not None:\n",
      "            root = os.path.expanduser(root)\n",
      "        if root is not None or extension is not None:\n",
      "            samples[label_id] = samples[label_id].map(\n",
      "                (lambda x: os.path.join(root, x + extension or ''))\n",
      "                if root is not None else (lambda x: x + extension)\n",
      "            )\n",
      "\n",
      "        classes = sorted(samples[label_target].unique())\n",
      "        if class_to_idx is None:\n",
      "            class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
      "        samples[label_target] = samples[label_target].map(lambda x: class_to_idx[x])\n",
      "\n",
      "        samples = samples.drop_duplicates()\n",
      "\n",
      "        self.ids = tuple(samples[label_id].drop_duplicates())\n",
      "        self.samples = samples\n",
      "        self.classes = classes\n",
      "        self.class_to_idx = class_to_idx\n",
      "        self.num_classes = len(class_to_idx)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.ids)\n",
      "\n",
      "    def __getitem__(self, index: int) -> ...:\n",
      "        image_id = self.ids[index]\n",
      "        row = self.samples[self.samples[self.label_id] == image_id]\n",
      "        path, target = row[self.label_id], row[self.label_target]\n",
      "        sample = self.loader(path.item())\n",
      "        if self.transform is not None:\n",
      "            sample = self.transform(sample)\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "        else:\n",
      "            target = np.array(target)\n",
      "        return sample, target\n",
      "\n",
      "class ImageFolder(_ImageFolder):\n",
      "    \"\"\"A generic data loader where the images are arranged in root folder.\n",
      "\n",
      "    Args:\n",
      "        root (string): Root directory path.\n",
      "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "        loader (callable, optional): A function to load an image given its path.\n",
      "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
      "            and check if the file is a valid file (used to check of corrupt files)\n",
      "\n",
      "     Attributes:\n",
      "        classes (list): List of the class names sorted alphabetically.\n",
      "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "        imgs (list): List of (image path, class_index) tuples\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "            self,\n",
      "            root: typing.Union[str, os.PathLike],\n",
      "            class_to_idx: typing.Optional[typing.Dict[str, int]] = None,\n",
      "            transform: typing.Optional[typing.Callable] = None,\n",
      "            target_transform: typing.Optional[typing.Callable] = None,\n",
      "            loader: typing.Callable[[str], typing.Any] = default_loader,\n",
      "            is_valid_file: typing.Optional[typing.Callable[[str], bool]] = None,\n",
      "    ):\n",
      "        self.class_to_idx = class_to_idx\n",
      "        super(ImageFolder, self).__init__(root, transform, target_transform, loader, is_valid_file)\n",
      "\n",
      "    def _find_classes(self, directory: str) -> typing.Tuple[typing.List[str], typing.Dict[str, int]]:\n",
      "        \"\"\"\n",
      "        Finds the class folders in a dataset.\n",
      "\n",
      "        Args:\n",
      "            directory (string): Root directory path.\n",
      "\n",
      "        Returns:\n",
      "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
      "\n",
      "        Ensures:\n",
      "            No class is a subdirectory of another.\n",
      "        \"\"\"\n",
      "        classes = [d.name for d in os.scandir(directory) if d.is_dir()]\n",
      "        classes.sort()\n",
      "        try:\n",
      "            class_to_idx = self.class_to_idx\n",
      "        except AttributeError:\n",
      "            class_to_idx = None\n",
      "        if class_to_idx is None:\n",
      "            class_to_idx = self.class_to_idx or {cls_name: i for i, cls_name in enumerate(classes)}\n",
      "        return classes, class_to_idx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset Class Source Code\n",
    "\n",
    "print(inspect.getsource(data_prep_utils.dataset.ImageWithPandas))\n",
    "print(inspect.getsource(data_prep_utils.dataset.ImageFolder))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomResizedCrop(256),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.45],std=[0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            # A.RandomSizedCrop(min_max_height=(1024, 1024), height=1024, width=1024, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2,\n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            # A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            # A.Resize(height=512, width=512, p=1),\n",
    "            # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=512, width=512, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "classification_dataset_1 = rsna_pneumonia_detection_challenge.torch_classification_dataset(data_transform)\n",
    "\n",
    "classification_dataset_2 = covid_19_radiography_dataset.torch_classification_dataset(data_transform)\n",
    "\n",
    "detection_dataset = rsna_pneumonia_detection_challenge.torch_detection_dataset(get_train_transforms())\n",
    "\n",
    "\n",
    "cls_dset = torch.utils.data.ConcatDataset([\n",
    "    classification_dataset_1,\n",
    "    classification_dataset_2\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Make data loader from dataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader_cls = torch.utils.data.DataLoader(cls_dset,\n",
    "                                              batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "train_loader_cls_1 = torch.utils.data.DataLoader(classification_dataset_1,\n",
    "                                              batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=2)\n",
    "train_loader_cls_2 = torch.utils.data.DataLoader(classification_dataset_2,\n",
    "                                              batch_size=batch_size, shuffle=True,\n",
    "                                              num_workers=2)\n",
    "\n",
    "train_loader_det_1 = torch.utils.data.DataLoader(\n",
    "        detection_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        # pin_memory=False,\n",
    "        num_workers=2\n",
    ")\n",
    "\n",
    "train_loaders = [train_loader_cls_1, train_loader_cls_2, train_loader_det_1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Design Model Architecture\n",
    "* Base Model: EfficientNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# Shared Feature Extractor\n",
    "feature_extractor_depth = 4\n",
    "# FIXME -> from pretrained\n",
    "feature_extractor = EfficientNet.from_name(f'efficientnet-b{feature_extractor_depth}', include_top=False)\n",
    "feature_extractor.out_channels = feature_extractor._bn1.num_features\n",
    "\n",
    "# Get stem static or dynamic convolution depending on image size\n",
    "image_size = global_params.image_size\n",
    "Conv2d = get_same_padding_conv2d(image_size=image_size)\n",
    "\n",
    "# Stem\n",
    "in_channels = 3  # rgb\n",
    "out_channels = round_filters(32, self._global_params)  # number of output channels\n",
    "self._conv_stem = Conv2d(in_channels, out_channels, kernel_size=3, stride=2, bias=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone, num_classes, out_channels=None, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        out_channels = out_channels or backbone.out_channels\n",
    "        self.feature_extractor = backbone\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Classifier(feature_extractor, 4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<timm.models.features.FeatureInfo object at 0x15c39f910>\n"
     ]
    }
   ],
   "source": [
    "detection_config = get_efficientdet_config('tf_efficientdet_d4')\n",
    "detection_config.update(num_classes=4)\n",
    "\n",
    "feature_backbone = tf_efficientnet_b4(\n",
    "    pretrained=False,  # FIXME\n",
    "    features_only=True,\n",
    "    out_indices=(2, 3, 4),\n",
    "    in_chans = 1,\n",
    "    **detection_config.backbone_args\n",
    ")\n",
    "print(feature_backbone.feature_info)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "class EfficientDet(nn.Module):\n",
    "\n",
    "    def __init__(self, config, backbone):\n",
    "        super(EfficientDet, self).__init__()\n",
    "        self.config = config\n",
    "        self.backbone = backbone\n",
    "        self.fpn = BiFpn(self.config, backbone.feature_info.get_dicts(keys=['num_chs', 'reduction']))\n",
    "        self.class_net = HeadNet(self.config, num_outputs=config.num_classes)  # num_classes\n",
    "        self.box_net = HeadNet(self.config, num_outputs=4)\n",
    "\n",
    "        for n, m in self.named_modules():\n",
    "            if 'backbone' not in n:\n",
    "                _init_weight(m, n)\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def toggle_head_bn_level_first(self):\n",
    "        \"\"\" Toggle the head batchnorm layers between being access with feature_level first vs repeat\n",
    "        \"\"\"\n",
    "        self.class_net.toggle_bn_level_first()\n",
    "        self.box_net.toggle_bn_level_first()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.fpn(x)\n",
    "        x_class = self.class_net(x)\n",
    "        x_box = self.box_net(x)\n",
    "        return x_class, x_box\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def detect(self, x):\n",
    "        return self.forward(x)[1]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def classify(self, x):\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "\n",
    "net = EfficientDet(detection_config, feature_backbone)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from train_utils import Trainer, MultipleOptimizerHandler\n",
    "\n",
    "\n",
    "class AdvancedFitter(Trainer):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model,\n",
    "            config,\n",
    "            opt_c,\n",
    "            opt_d,\n",
    "            epoch: int,\n",
    "            train_iter = None,\n",
    "            val_iter = None,\n",
    "            test_iter = None,\n",
    "            snapshot_dir = None,\n",
    "            verbose: bool = True,\n",
    "            timer: bool = False,\n",
    "            log_interval = 20,\n",
    "    ) -> None:\n",
    "\n",
    "        self.train_iter = train_iter\n",
    "        self.val_iter = val_iter\n",
    "        self.test_iter = test_iter\n",
    "\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.criterion = DetBenchTrain(model, config)\n",
    "        self.optimizer = MultipleOptimizerHandler({'c': opt_c, 'd': opt_d})\n",
    "        self.total_epoch: int = epoch\n",
    "        self.snapshot_dir: pathlib.Path = pathlib.Path(snapshot_dir).resolve()\n",
    "        self.verbose: bool = verbose\n",
    "        self.use_timer: bool = timer\n",
    "        self.log_interval: int = log_interval\n",
    "        self.save_and_load: bool = bool(snapshot_dir is not None and val_iter is not None)\n",
    "\n",
    "        # FIXME\n",
    "        # self.train_batch_size: int = train_iter.batch_size\n",
    "        # self.train_loader_length: int = len(train_iter)\n",
    "        # self.train_dataset_length: int = len(getattr(train_iter, 'dataset', train_iter))\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Do not set attribute of instance.\n",
    "        print(\"Advanced Fitter Initialized.\")\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        self._require_context()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        verbose = self.verbose\n",
    "        log_interval = self.log_interval\n",
    "\n",
    "        total_loss = total_accuracy = 0.\n",
    "        total_batch = 0\n",
    "        det_loss = 0.\n",
    "        det_batch = 0\n",
    "\n",
    "        datasets = self.train_iter\n",
    "\n",
    "        for data in datasets:\n",
    "\n",
    "            whole = len(data)\n",
    "            for iteration, (images, targets) in enumerate(data, 1):\n",
    "\n",
    "                if isinstance(targets, dict):\n",
    "                    l = self._train_detection(images, targets)\n",
    "                    det_loss += l; det_batch += 1\n",
    "                    if iteration % log_interval == 0 and verbose:\n",
    "                        self._log_train_doing(l, iteration, whole)\n",
    "\n",
    "                else:\n",
    "                    l, a = self._train_classification(images, targets)\n",
    "                    total_loss += l; total_accuracy += a; total_batch += 1\n",
    "                    if iteration % log_interval == 0 and verbose:\n",
    "                        self._log_train_doing(l, iteration, whole)\n",
    "\n",
    "        avg_loss = total_loss / total_batch\n",
    "        avg_accuracy = total_accuracy / total_batch\n",
    "\n",
    "        self._log_train_done(avg_loss, avg_accuracy)\n",
    "\n",
    "        det_avg_loss = det_loss / det_batch\n",
    "\n",
    "        self._log_train_done(det_avg_loss)\n",
    "\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def _train_classification(self, images, targets):\n",
    "\n",
    "        images = self._to_apply_tensor(images).float()\n",
    "        prediction = self.model(images)[0]\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, targets)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l = loss.item()\n",
    "            a = torch.eq(torch.argmax(prediction, 1), targets).float().mean().item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer = self.optimizer['c']\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        return l, a\n",
    "\n",
    "    def _train_detection(self, images, targets):\n",
    "\n",
    "        images = self._to_apply_tensor(images).float()\n",
    "        boxes = [self._to_apply_tensor(target['boxes']).float() for target in targets]\n",
    "        labels = [self._to_apply_tensor(target['labels']).float() for target in targets]\n",
    "\n",
    "        loss, _, _ = self.criterion(images, boxes, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer = self.optimizer['d']\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _evaluate(self, *, test=False):\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        datasets = self.test_iter if test else self.val_iter\n",
    "\n",
    "        total_loss = total_accuracy = 0.\n",
    "        total_batch = 0\n",
    "        det_loss = 0.\n",
    "        det_batch = 0\n",
    "\n",
    "        for data in datasets:\n",
    "\n",
    "            for images, targets in data:\n",
    "\n",
    "                if isinstance(targets, dict):\n",
    "                    l = self._eval_detection(images, targets)\n",
    "                    det_loss += l; det_batch += 1\n",
    "\n",
    "                else:\n",
    "                    l, a = self._eval_classification(images, targets)\n",
    "                    total_loss += l; total_accuracy += a; total_batch += 1\n",
    "\n",
    "        avg_loss = total_loss / total_batch\n",
    "        avg_accuracy = total_accuracy / total_batch\n",
    "\n",
    "        self._log_eval(avg_loss, avg_accuracy, test=test)\n",
    "\n",
    "        det_avg_loss = det_loss / det_batch\n",
    "\n",
    "        self._log_eval(det_avg_loss, test=test)\n",
    "\n",
    "        return avg_loss, avg_accuracy\n",
    "\n",
    "    def _eval_classification(self, images, targets):\n",
    "\n",
    "        images = self._to_apply_tensor(images).float()\n",
    "        prediction = self.model(images)[0]\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, targets)\n",
    "\n",
    "        l = loss.item()\n",
    "        a = torch.eq(torch.argmax(prediction, 1), targets).float().mean().item()\n",
    "\n",
    "        return l, a\n",
    "\n",
    "    def _eval_detection(self, images, targets):\n",
    "\n",
    "        images = self._to_apply_tensor(images).float()\n",
    "        boxes = [self._to_apply_tensor(target['boxes']).float() for target in targets]\n",
    "        labels = [self._to_apply_tensor(target['labels']).float() for target in targets]\n",
    "\n",
    "        loss, _, _ = self.criterion(images, boxes, labels)\n",
    "\n",
    "        return loss.item()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "dset = ConcatDataset([\n",
    "    classification_dataset_1,\n",
    "    classification_dataset_2\n",
    "])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Start Learning> \t\t\t\tTotal 3 epochs\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "<Stop Learning> \tLeast loss: inf\tDuration: 06.44s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [48, 3, 3, 3], expected input[64, 1, 257, 257] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/0n/z3tlqlk136v990dbw2gx0rkc0000gn/T/ipykernel_46397/2134067847.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m ) as trainer:\n\u001B[1;32m     14\u001B[0m     \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'cuda'\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m'cpu'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m     \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Projects/Python/Chest-X-Ray-Detection/train_utils/__init__.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    428\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    429\u001B[0m                 \u001B[0;32mwhile\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_current_epoch\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtotal_epoch\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 430\u001B[0;31m                     \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    431\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    432\u001B[0m             \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Projects/Python/Chest-X-Ray-Detection/train_utils/__init__.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    395\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_current_epoch\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 397\u001B[0;31m         \u001B[0mtrain_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_train\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    398\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_save\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Projects/Python/Chest-X-Ray-Detection/train_utils/__init__.py\u001B[0m in \u001B[0;36m_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    661\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0miteration\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_iter\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    662\u001B[0m             \u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_to_apply_multi_tensor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 663\u001B[0;31m             \u001B[0mprediction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    664\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcriterion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    665\u001B[0m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/0n/z3tlqlk136v990dbw2gx0rkc0000gn/T/ipykernel_46397/2927673061.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature_extractor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/efficientnet_pytorch/model.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    312\u001B[0m         \"\"\"\n\u001B[1;32m    313\u001B[0m         \u001B[0;31m# Convolution layers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 314\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextract_features\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    315\u001B[0m         \u001B[0;31m# Pooling and final linear layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    316\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_avg_pooling\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/efficientnet_pytorch/model.py\u001B[0m in \u001B[0;36mextract_features\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    287\u001B[0m         \"\"\"\n\u001B[1;32m    288\u001B[0m         \u001B[0;31m# Stem\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 289\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_swish\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bn0\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conv_stem\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    290\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m         \u001B[0;31m# Blocks\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    888\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 889\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[1;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/python-dl/lib/python3.8/site-packages/efficientnet_pytorch/utils.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    273\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    274\u001B[0m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstatic_padding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 275\u001B[0;31m         \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconv2d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdilation\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    276\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given groups=1, weight of size [48, 3, 3, 3], expected input[64, 1, 257, 257] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "with train_utils.Trainer(\n",
    "        model, nn.CrossEntropyLoss(), optimizer, num_epochs,\n",
    "        train_loader_cls_2, train_loader_cls_2,\n",
    "        snapshot_dir = 'snapshots',\n",
    "        verbose = True,\n",
    "        timer = True,\n",
    ") as trainer:\n",
    "    trainer.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    trainer.run()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fitter = AdvancedFitter(\n",
    "#     net, detection_config, optimizer, optimizer,\n",
    "#     num_epochs,\n",
    "#     train_iter = train_loaders,\n",
    "#     val_iter = train_loaders,\n",
    "#     snapshot_dir = 'snapshots',\n",
    "#     verbose=True,\n",
    "#     timer=True,\n",
    "# )\n",
    "# fitter.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "# with fitter:\n",
    "#     fitter.run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}